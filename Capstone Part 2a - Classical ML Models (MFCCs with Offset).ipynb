{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Part 2a - Classical ML Models (MFCCs with Offset)\n",
    "___\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# For splitting the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# For scaling the data as necessary\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For doing principal component analysis as necessary\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# For visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# For building a variety of models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# For hyperparameter optimization\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# For caching pipeline and grid search results\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "# For model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# For getting rid of warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For pickling models\n",
    "import joblib\n",
    "\n",
    "# Loading in the finished dataframe from part 1\n",
    "ravdess_mfcc_df = pd.read_csv('C:/Users/Patrick/Documents/Capstone Data/ravdess_mfcc.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Building Models for Classifying Gender (Regardless of Emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe into features and target\n",
    "X = ravdess_mfcc_df.iloc[:, :-2]\n",
    "g = ravdess_mfcc_df['Gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convention is to name the target variable 'y', but I will be declaring many different target variables throughout the notebook, so I opted for 'g' for simplicity instead of 'y_g' or 'y_gen', for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoding the genders\n",
    "# gender_encoder = LabelEncoder()\n",
    "# g = gender_encoder.fit_transform(g)\n",
    "\n",
    "# # Checking the results\n",
    "# g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Which number represents which gender?\n",
    "# for num in np.unique(g):\n",
    "#     print(f'{num} represents {gender_encoder.inverse_transform([num])[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I realized that encoding the target is unnecessary; it is done automatically by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of g: 1438\n",
      "30% of 1438 is 431.4\n"
     ]
    }
   ],
   "source": [
    "# What test size should I use?\n",
    "print(f'Length of g: {len(g)}')\n",
    "print(f'30% of {len(g)} is {len(g)*0.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, g_train, g_test = train_test_split(X, g, test_size=0.3, stratify=g, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1006, 2600)\n",
      "(432, 2600)\n",
      "(1006,)\n",
      "(432,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shapes\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(g_train.shape)\n",
    "print(g_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to build a simple, initial classifier to get a sense of the performances I might get in more optimized models. To this end, I will build a logistic regression model without doing any cross-validation or hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on training set: 100.0%\n",
      "Model accuracy on test set: 99.07407407407408%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "initial_logreg = LogisticRegression()\n",
    "\n",
    "# Fit to training set\n",
    "initial_logreg.fit(X_train, g_train)\n",
    "\n",
    "# Score on training set\n",
    "print(f'Model accuracy on training set: {initial_logreg.score(X_train, g_train)*100}%')\n",
    "\n",
    "# Score on test set\n",
    "print(f'Model accuracy on test set: {initial_logreg.score(X_test, g_test)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are extremely high accuracies. The model has most likely overfit to the training set, but the accuracy on the test set is still surprisingly high.\n",
    "\n",
    "Here are some possible explanations:\n",
    "- The dataset (RAVDESS) is relatively small, with only 1440 data points (1438 if I do not count the two very short clips that I excluded). This model is likely not very robust and has easily overfit to the training set.\n",
    "- The features I have extracted could be excellent predictors of gender.\n",
    "- This could be a very simple classification task. After all, there are only two classes, and theoretically, features extracted from male and female voice clips should have distinguishable patterns.\n",
    "\n",
    "I had originally planned to build more gender classification models for this dataset, but I will forgo this for now. In part 4, I will try using this model to classify clips from another dataset and examine its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickle1_gender_logreg.pkl']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickling the model for later use\n",
    "joblib.dump(initial_logreg, 'pickle1_gender_logreg.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Building Models for Classifying Emotion for Males"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2592</th>\n",
       "      <th>2593</th>\n",
       "      <th>2594</th>\n",
       "      <th>2595</th>\n",
       "      <th>2596</th>\n",
       "      <th>2597</th>\n",
       "      <th>2598</th>\n",
       "      <th>2599</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.165711</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>-857.309453</td>\n",
       "      <td>...</td>\n",
       "      <td>1.546741</td>\n",
       "      <td>1.982412</td>\n",
       "      <td>2.188718</td>\n",
       "      <td>2.742868</td>\n",
       "      <td>0.987672</td>\n",
       "      <td>1.042513</td>\n",
       "      <td>1.416731</td>\n",
       "      <td>0.817693</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-864.890286</td>\n",
       "      <td>-864.890286</td>\n",
       "      <td>-864.890286</td>\n",
       "      <td>-864.890286</td>\n",
       "      <td>-861.939471</td>\n",
       "      <td>-860.492254</td>\n",
       "      <td>-863.877686</td>\n",
       "      <td>-862.146948</td>\n",
       "      <td>-860.825373</td>\n",
       "      <td>-862.288360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.760483</td>\n",
       "      <td>0.900741</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-849.445433</td>\n",
       "      <td>-848.003746</td>\n",
       "      <td>-844.977418</td>\n",
       "      <td>-848.275738</td>\n",
       "      <td>-847.952191</td>\n",
       "      <td>-847.026382</td>\n",
       "      <td>-849.022875</td>\n",
       "      <td>-848.187830</td>\n",
       "      <td>-851.179822</td>\n",
       "      <td>-852.391785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056836</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-832.734397</td>\n",
       "      <td>-836.552680</td>\n",
       "      <td>-834.580541</td>\n",
       "      <td>-827.005858</td>\n",
       "      <td>-830.058798</td>\n",
       "      <td>-831.195983</td>\n",
       "      <td>-839.331695</td>\n",
       "      <td>-838.307510</td>\n",
       "      <td>-834.128916</td>\n",
       "      <td>-839.040966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098864</td>\n",
       "      <td>0.972775</td>\n",
       "      <td>1.156853</td>\n",
       "      <td>1.185786</td>\n",
       "      <td>0.808819</td>\n",
       "      <td>0.931570</td>\n",
       "      <td>0.595077</td>\n",
       "      <td>-0.121792</td>\n",
       "      <td>male</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.148748</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>-903.185111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217088</td>\n",
       "      <td>1.191406</td>\n",
       "      <td>1.323715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.630213</td>\n",
       "      <td>0.166922</td>\n",
       "      <td>0.312804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>713</td>\n",
       "      <td>-704.004406</td>\n",
       "      <td>-703.014570</td>\n",
       "      <td>-702.031038</td>\n",
       "      <td>-703.328468</td>\n",
       "      <td>-703.912309</td>\n",
       "      <td>-703.765455</td>\n",
       "      <td>-704.655862</td>\n",
       "      <td>-704.481680</td>\n",
       "      <td>-703.788671</td>\n",
       "      <td>-702.783204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.045276</td>\n",
       "      <td>0.192656</td>\n",
       "      <td>0.852603</td>\n",
       "      <td>0.690963</td>\n",
       "      <td>male</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>714</td>\n",
       "      <td>-696.685285</td>\n",
       "      <td>-696.643536</td>\n",
       "      <td>-696.814208</td>\n",
       "      <td>-696.310705</td>\n",
       "      <td>-695.800097</td>\n",
       "      <td>-696.338568</td>\n",
       "      <td>-697.214325</td>\n",
       "      <td>-697.745507</td>\n",
       "      <td>-697.431831</td>\n",
       "      <td>-697.506973</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059871</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.120239</td>\n",
       "      <td>1.189020</td>\n",
       "      <td>0.927918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>-692.801891</td>\n",
       "      <td>-692.803866</td>\n",
       "      <td>-693.082432</td>\n",
       "      <td>-693.371465</td>\n",
       "      <td>-693.371465</td>\n",
       "      <td>-693.371465</td>\n",
       "      <td>-693.371465</td>\n",
       "      <td>-693.351801</td>\n",
       "      <td>-692.287004</td>\n",
       "      <td>-692.734244</td>\n",
       "      <td>...</td>\n",
       "      <td>0.668347</td>\n",
       "      <td>0.486409</td>\n",
       "      <td>0.601826</td>\n",
       "      <td>0.474456</td>\n",
       "      <td>0.194573</td>\n",
       "      <td>0.030500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.118997</td>\n",
       "      <td>male</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>-669.158136</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375527</td>\n",
       "      <td>0.331931</td>\n",
       "      <td>0.036515</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-692.916251</td>\n",
       "      <td>-692.413267</td>\n",
       "      <td>-692.632421</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>-693.055253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.098310</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.231602</td>\n",
       "      <td>0.421240</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>male</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>718 rows Ã— 2602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5  \\\n",
       "0   -857.309453 -857.309453 -857.309453 -857.309453 -857.309453 -857.309453   \n",
       "1   -864.890286 -864.890286 -864.890286 -864.890286 -861.939471 -860.492254   \n",
       "2   -849.445433 -848.003746 -844.977418 -848.275738 -847.952191 -847.026382   \n",
       "3   -832.734397 -836.552680 -834.580541 -827.005858 -830.058798 -831.195983   \n",
       "4   -903.185111 -903.185111 -903.185111 -903.185111 -903.185111 -903.185111   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "713 -704.004406 -703.014570 -702.031038 -703.328468 -703.912309 -703.765455   \n",
       "714 -696.685285 -696.643536 -696.814208 -696.310705 -695.800097 -696.338568   \n",
       "715 -692.801891 -692.803866 -693.082432 -693.371465 -693.371465 -693.371465   \n",
       "716 -669.158136 -669.158136 -669.158136 -669.158136 -669.158136 -669.158136   \n",
       "717 -693.055253 -693.055253 -693.055253 -692.916251 -692.413267 -692.632421   \n",
       "\n",
       "              6           7           8           9  ...      2592      2593  \\\n",
       "0   -857.309453 -857.165711 -857.309453 -857.309453  ...  1.546741  1.982412   \n",
       "1   -863.877686 -862.146948 -860.825373 -862.288360  ...  0.760483  0.900741   \n",
       "2   -849.022875 -848.187830 -851.179822 -852.391785  ...  0.000000  0.000000   \n",
       "3   -839.331695 -838.307510 -834.128916 -839.040966  ...  0.098864  0.972775   \n",
       "4   -903.148748 -903.185111 -903.185111 -903.185111  ...  0.217088  1.191406   \n",
       "..          ...         ...         ...         ...  ...       ...       ...   \n",
       "713 -704.655862 -704.481680 -703.788671 -702.783204  ...  0.000000  0.000000   \n",
       "714 -697.214325 -697.745507 -697.431831 -697.506973  ...  0.000000  0.059871   \n",
       "715 -693.371465 -693.351801 -692.287004 -692.734244  ...  0.668347  0.486409   \n",
       "716 -669.158136 -669.158136 -669.158136 -669.158136  ...  0.375527  0.331931   \n",
       "717 -693.055253 -693.055253 -693.055253 -693.055253  ...  0.098310  0.000184   \n",
       "\n",
       "         2594      2595      2596      2597      2598      2599  Gender  \\\n",
       "0    2.188718  2.742868  0.987672  1.042513  1.416731  0.817693    male   \n",
       "1    0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    male   \n",
       "2    0.000000  0.000000  0.000000  0.056836  0.000000  0.000000    male   \n",
       "3    1.156853  1.185786  0.808819  0.931570  0.595077 -0.121792    male   \n",
       "4    1.323715  0.000000  0.630213  0.166922  0.312804  0.000000    male   \n",
       "..        ...       ...       ...       ...       ...       ...     ...   \n",
       "713  0.000000  0.000000  0.045276  0.192656  0.852603  0.690963    male   \n",
       "714  0.004736  0.120239  1.189020  0.927918  0.000000  0.000000    male   \n",
       "715  0.601826  0.474456  0.194573  0.030500  0.000000  0.118997    male   \n",
       "716  0.036515  0.000000  0.000000  0.000000  0.000000  0.000000    male   \n",
       "717  0.231602  0.421240  0.000000  0.000000  0.000000  0.000000    male   \n",
       "\n",
       "       Emotion  \n",
       "0      neutral  \n",
       "1      neutral  \n",
       "2      neutral  \n",
       "3      neutral  \n",
       "4         calm  \n",
       "..         ...  \n",
       "713  surprised  \n",
       "714  surprised  \n",
       "715  surprised  \n",
       "716  surprised  \n",
       "717  surprised  \n",
       "\n",
       "[718 rows x 2602 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a new dataframe that contains only male recordings\n",
    "ravdess_mfcc_m_df = ravdess_mfcc_df[ravdess_mfcc_df['Gender'] == 'male'].reset_index().drop('index', axis=1)\n",
    "ravdess_mfcc_m_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe into features and target\n",
    "Xm = ravdess_mfcc_m_df.iloc[:, :-2]\n",
    "em = ravdess_mfcc_m_df['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Encoding the emotions\n",
    "# emotion_encoder = LabelEncoder()\n",
    "# em = emotion_encoder.fit_transform(em)\n",
    "\n",
    "# # Checking the results\n",
    "# em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Which number represents which emotion?\n",
    "# for num in np.unique(em):\n",
    "#     print(f'{num} represents {emotion_encoder.inverse_transform([num])[0]}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I realized that encoding the target is unnecessary; it is done automatically by the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "Xm_train, Xm_test, em_train, em_test = train_test_split(Xm, em, test_size=0.3, stratify=em, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(502, 2600)\n",
      "(216, 2600)\n",
      "(502,)\n",
      "(216,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shapes\n",
    "print(Xm_train.shape)\n",
    "print(Xm_test.shape)\n",
    "print(em_train.shape)\n",
    "print(em_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, I will try building an initial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on training set: 100.0%\n",
      "Model accuracy on test set: 56.481481481481474%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "initial_logreg_em = LogisticRegression()\n",
    "\n",
    "# Fit to training set\n",
    "initial_logreg_em.fit(Xm_train, em_train)\n",
    "\n",
    "# Score on training set\n",
    "print(f'Model accuracy on training set: {initial_logreg_em.score(Xm_train, em_train)*100}%')\n",
    "\n",
    "# Score on test set\n",
    "print(f'Model accuracy on test set: {initial_logreg_em.score(Xm_test, em_test)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has overfit to the training set yet again, and this time the accuracy on the test set leaves a lot to be desired. Let's evaluate the model further using a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted angry</th>\n",
       "      <th>Predicted calm</th>\n",
       "      <th>Predicted disgusted</th>\n",
       "      <th>Predicted fearful</th>\n",
       "      <th>Predicted happy</th>\n",
       "      <th>Predicted neutral</th>\n",
       "      <th>Predicted sad</th>\n",
       "      <th>Predicted surprised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual angry</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual calm</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual disgusted</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual fearful</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual happy</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual sad</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual surprised</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted angry  Predicted calm  Predicted disgusted  \\\n",
       "Actual angry                   24               2                    0   \n",
       "Actual calm                     0              26                    1   \n",
       "Actual disgusted                4               2                   13   \n",
       "Actual fearful                  3               1                    1   \n",
       "Actual happy                    6               0                    3   \n",
       "Actual neutral                  0               3                    0   \n",
       "Actual sad                      0               4                    3   \n",
       "Actual surprised                2               1                    0   \n",
       "\n",
       "                  Predicted fearful  Predicted happy  Predicted neutral  \\\n",
       "Actual angry                      2                0                  1   \n",
       "Actual calm                       0                0                  0   \n",
       "Actual disgusted                  3                1                  3   \n",
       "Actual fearful                   20                2                  0   \n",
       "Actual happy                      2               13                  3   \n",
       "Actual neutral                    0                2                  5   \n",
       "Actual sad                        1                7                  2   \n",
       "Actual surprised                  4                6                  2   \n",
       "\n",
       "                  Predicted sad  Predicted surprised  \n",
       "Actual angry                  0                    0  \n",
       "Actual calm                   1                    0  \n",
       "Actual disgusted              2                    1  \n",
       "Actual fearful                2                    0  \n",
       "Actual happy                  0                    2  \n",
       "Actual neutral                2                    2  \n",
       "Actual sad                   10                    2  \n",
       "Actual surprised              3                   11  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having initial_logreg_em make predictions based on the test set features\n",
    "em_pred = initial_logreg_em.predict(Xm_test)\n",
    "\n",
    "# Building the confusion matrix as a dataframe\n",
    "emotions = ['angry', 'calm', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "em_confusion_df = pd.DataFrame(confusion_matrix(em_test, em_pred))\n",
    "em_confusion_df.columns = [f'Predicted {emotion}' for emotion in emotions]\n",
    "em_confusion_df.index = [f'Actual {emotion}' for emotion in emotions]\n",
    "em_confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.62      0.83      0.71        29\n",
      "        calm       0.67      0.93      0.78        28\n",
      "   disgusted       0.62      0.45      0.52        29\n",
      "     fearful       0.62      0.69      0.66        29\n",
      "       happy       0.42      0.45      0.43        29\n",
      "     neutral       0.31      0.36      0.33        14\n",
      "         sad       0.50      0.34      0.41        29\n",
      "   surprised       0.61      0.38      0.47        29\n",
      "\n",
      "    accuracy                           0.56       216\n",
      "   macro avg       0.55      0.55      0.54       216\n",
      "weighted avg       0.56      0.56      0.55       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(em_test, em_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a binary classification problem, there is one negative class and one positive class. This is not the case here, because this is a multiclass classification problem. In the table above, each row of precision and recall scores assumes the corresponding emotion is the positive class, and groups all other emotions as the negative class.\n",
    "\n",
    "Precision is the following measure: Of all the data points that the model classified as belonging to the positive class (i.e., the true and false positives), what proportion is correct (i.e., truly positive)?\n",
    "\n",
    "Recall is the following measure: Of all the data points that are truly positive (i.e., the true positives and false negatives as classified by the model), what proportion did the model correctly classify (i.e., the true positives)?\n",
    "\n",
    "It appears that the initial model is strongest at classifying calm voice clips, and weakest at classifying neutral voice clips. In order of strongest to weakest: calm, angry, fearful, disgusted, surprised, happy, sad, and neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now try building new models and optimizing hyperparameters to obtain better performance. I will use a pipeline and multiple grid searches to accomplish this.\n",
    "\n",
    "Before I build all my models in bulk, I want to see if doing principal component analysis (PCA) could be beneficial. I will do PCA on both unscaled and scaled features, and plot the resulting explained variance ratios. I have two goals here:\n",
    "- Get a sense of whether scaling would be beneficial for model performance\n",
    "- Get a sense of how many principal components I should use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on unscaled features\n",
    "\n",
    "# Instantiate PCA and fit to Xm_train\n",
    "pca = PCA().fit(Xm_train)\n",
    "\n",
    "# Transform Xm_train\n",
    "Xm_train_pca = pca.transform(Xm_train)\n",
    "\n",
    "# Transform Xm_test\n",
    "Xm_test_pca = pca.transform(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling\n",
    "\n",
    "# Instantiate the scaler and fit to Xm_train\n",
    "scaler = StandardScaler().fit(Xm_train)\n",
    "\n",
    "# Transform Xm_train\n",
    "Xm_train_scaled = scaler.transform(Xm_train)\n",
    "\n",
    "# Transform Xm_test\n",
    "Xm_test_scaled = scaler.transform(Xm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on scaled features\n",
    "\n",
    "# Instantiate PCA and fit to Xm_train_scaled\n",
    "pca_scaled = PCA().fit(Xm_train_scaled)\n",
    "\n",
    "# Transform Xm_train_scaled\n",
    "Xm_train_scaled_pca = pca_scaled.transform(Xm_train_scaled)\n",
    "\n",
    "# Transform Xm_test_scaled\n",
    "Xm_test_scaled_pca = pca_scaled.transform(Xm_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the explained variance ratios\n",
    "\n",
    "plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "# Unscaled\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(np.arange(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA on Unscaled Features')\n",
    "plt.ylim(top = 0.5) # Equalizing the y-axes\n",
    "\n",
    "# Scaled\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(np.arange(1, len(pca_scaled.explained_variance_ratio_)+1), pca_scaled.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA on Scaled Features')\n",
    "plt.ylim(top = 0.5) # Equalizing the y-axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal components are linear combinations of the original features, ordered by how much of the dataset's variance they explain. Looking at the two plots above, it appears that for the same number of principal components, those using unscaled features are able to explain more variance (i.e., capture more information) than those using scaled features. For example, looking at the first ~25 principal components of each plot, the bars of the left plot (unscaled) are higher and skewed more to the left than those of the right plot (scaled). Since the purpose of PCA is to reduce dimensionality of the data by keeping the components that explain the most variance and discarding the rest, the unscaled principal components might benefit my models more than the scaled principal components will.\n",
    "\n",
    "However, I have to be mindful of the underlying variance in my features. Some features have values in the -800s, while others are close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2590</th>\n",
       "      <th>2591</th>\n",
       "      <th>2592</th>\n",
       "      <th>2593</th>\n",
       "      <th>2594</th>\n",
       "      <th>2595</th>\n",
       "      <th>2596</th>\n",
       "      <th>2597</th>\n",
       "      <th>2598</th>\n",
       "      <th>2599</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12741.942262</td>\n",
       "      <td>12742.838224</td>\n",
       "      <td>12924.214929</td>\n",
       "      <td>13234.300086</td>\n",
       "      <td>13497.107563</td>\n",
       "      <td>13640.969941</td>\n",
       "      <td>13645.733372</td>\n",
       "      <td>13607.47741</td>\n",
       "      <td>13811.048599</td>\n",
       "      <td>14075.089321</td>\n",
       "      <td>...</td>\n",
       "      <td>23.046341</td>\n",
       "      <td>21.620106</td>\n",
       "      <td>20.985742</td>\n",
       "      <td>19.501081</td>\n",
       "      <td>18.778024</td>\n",
       "      <td>18.449462</td>\n",
       "      <td>18.357353</td>\n",
       "      <td>19.298583</td>\n",
       "      <td>15.044105</td>\n",
       "      <td>14.07836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 2600 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4  \\\n",
       "0  12741.942262  12742.838224  12924.214929  13234.300086  13497.107563   \n",
       "\n",
       "              5             6            7             8             9  ...  \\\n",
       "0  13640.969941  13645.733372  13607.47741  13811.048599  14075.089321  ...   \n",
       "\n",
       "        2590       2591       2592       2593       2594       2595  \\\n",
       "0  23.046341  21.620106  20.985742  19.501081  18.778024  18.449462   \n",
       "\n",
       "        2596       2597       2598      2599  \n",
       "0  18.357353  19.298583  15.044105  14.07836  \n",
       "\n",
       "[1 rows x 2600 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining the variances\n",
    "var_df = pd.DataFrame(ravdess_mfcc_m_df.var()).T\n",
    "var_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since PCA is looking for high variance directions, it can become biased by the underlying variance in a given feature if I do not scale it down first. I can see that some features have much higher variance than others do, so there is likely a lot of bias in the unscaled principal components above.\n",
    "\n",
    "How much variance is explained by certain numbers of unscaled and scaled principal components? This will help me determine how many principal components to try in my grid searches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by 502 unscaled principal components: 100.0%\n",
      "Variance explained by 450 unscaled principal components: 99.94%\n",
      "Variance explained by 400 unscaled principal components: 99.84%\n",
      "Variance explained by 350 unscaled principal components: 99.69%\n",
      "Variance explained by 300 unscaled principal components: 99.47%\n",
      "Variance explained by 250 unscaled principal components: 99.14%\n",
      "Variance explained by 200 unscaled principal components: 98.63%\n",
      "Variance explained by 150 unscaled principal components: 97.84%\n",
      "Variance explained by 100 unscaled principal components: 96.43%\n",
      "Variance explained by 50 unscaled principal components: 93.15%\n"
     ]
    }
   ],
   "source": [
    "# Unscaled\n",
    "num_components = [503, 451, 401, 351, 301, 251, 201, 151, 101, 51]\n",
    "for n in num_components:\n",
    "    print(f'Variance explained by {n-1} unscaled principal components: {np.round(np.sum(pca.explained_variance_ratio_[:n])*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by 502 scaled principal components: 100.0%\n",
      "Variance explained by 450 scaled principal components: 99.52%\n",
      "Variance explained by 400 scaled principal components: 98.71%\n",
      "Variance explained by 350 scaled principal components: 97.51%\n",
      "Variance explained by 300 scaled principal components: 95.79%\n",
      "Variance explained by 250 scaled principal components: 93.36%\n",
      "Variance explained by 200 scaled principal components: 89.86%\n",
      "Variance explained by 150 scaled principal components: 84.76%\n",
      "Variance explained by 100 scaled principal components: 76.91%\n",
      "Variance explained by 50 scaled principal components: 63.35%\n"
     ]
    }
   ],
   "source": [
    "# Scaled\n",
    "num_components = [503, 451, 401, 351, 301, 251, 201, 151, 101, 51]\n",
    "for n in num_components:\n",
    "    print(f'Variance explained by {n-1} scaled principal components: {np.round(np.sum(pca_scaled.explained_variance_ratio_[:n])*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now build a pipeline and multiple grid searches with five-fold cross-validation to optimize the hyperparameters. I will try five types of classifiers: logistic regression, support vector machine, random forest, XGBoost, and k-nearest neighbours. To get a better sense of how each type performs, I will make a grid search for each one. I will also try different numbers of principal components for unscaled and scaled features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Pipeline (these values are placeholders)\n",
    "my_pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory=cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for log reg\n",
    "logreg_param_grid = [\n",
    "    # l1 without PCA\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [LogisticRegression(penalty='l1', n_jobs=-1)],\n",
    "     'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l1 unscaled with PCA\n",
    "    # 5 PCAs * 9 regularization strengths = 45 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 251, 50),\n",
    "     'model': [LogisticRegression(penalty='l1', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l1 scaled with PCA\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50),\n",
    "     'model': [LogisticRegression(penalty='l1', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) without PCA\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)],\n",
    "     'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) unscaled with PCA\n",
    "    # 5 PCAs * 9 regularization strengths = 45 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 251, 50),\n",
    "     'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) scaled with PCA\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50),\n",
    "     'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "logreg_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=logreg_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 198 candidates, totalling 990 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    6.5s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   36.5s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   53.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 990 out of 990 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the log reg grid search\n",
    "fitted_logreg_grid_em = logreg_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmp48i_ocis',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer', None),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best log reg?\n",
    "fitted_logreg_grid_em.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best log reg's accuracy on the training set: 100.0%\n",
      "The best log reg's accuracy on the test set: 58.79629629629629%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best log reg's accuracy on the training set: {fitted_logreg_grid_em.score(Xm_train, em_train)*100}%\")\n",
    "print(f\"The best log reg's accuracy on the test set: {fitted_logreg_grid_em.score(Xm_test, em_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickle2_male_emotion_logreg.pkl']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickling the best log reg for later use\n",
    "joblib.dump(fitted_logreg_grid_em.best_estimator_, 'pickle2_male_emotion_logreg.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for SVM\n",
    "svm_param_grid = [\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [SVC()], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # unscaled\n",
    "    # 5 PCAs * 9 regularization strengths = 45 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 251, 50), 'model': [SVC()],\n",
    "     'model__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # scaled\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50), 'model': [SVC()],\n",
    "     'model__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "]\n",
    "\n",
    "# Instantiate the SVM grid search\n",
    "svm_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=svm_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 99 candidates, totalling 495 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   33.1s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   49.5s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   58.4s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 495 out of 495 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the SVM grid search\n",
    "fitted_svm_grid_em = svm_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmp48i_ocis',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer', None),\n",
       "                ('model',\n",
       "                 SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best SVM?\n",
    "fitted_svm_grid_em.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best SVM's accuracy on the training set: 100.0%\n",
      "The best SVM's accuracy on the test set: 60.18518518518518%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best SVM's accuracy on the training set: {fitted_svm_grid_em.score(Xm_train, em_train)*100}%\")\n",
    "print(f\"The best SVM's accuracy on the test set: {fitted_svm_grid_em.score(Xm_test, em_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pickle3_male_emotion_svm.pkl']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pickling the best SVM for later use\n",
    "joblib.dump(fitted_svm_grid_em.best_estimator_, 'pickle3_male_emotion_svm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for random forest (scaling is unnecessary)\n",
    "rf_param_grid = [\n",
    "    # 5 numbers of estimators * 5 max depths = 25 models\n",
    "    {'scaler': [None], 'dim_reducer': [None], 'model': [RandomForestClassifier(n_jobs=-1)], 'model__n_estimators': np.arange(100, 501, 100),\n",
    "     'model__max_depth': np.arange(5, 26, 5)},\n",
    "    \n",
    "    # 5 PCAs * 5 numbers of estimators * 5 max depths = 150 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 251, 50), 'model': [RandomForestClassifier(n_jobs=-1)],\n",
    "     'model__n_estimators': np.arange(100, 501, 100), 'model__max_depth': np.arange(5, 26, 5)}\n",
    "]\n",
    "\n",
    "# Instantiate the rf grid search\n",
    "rf_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=rf_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 150 candidates, totalling 750 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.8s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  6.3min\n",
      "[Parallel(n_jobs=-1)]: Done 750 out of 750 | elapsed:  7.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the rf grid search\n",
    "fitted_rf_grid_em = rf_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmplqvl750x',\n",
       "         steps=[('scaler', None),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=150,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=25,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=200, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best rf?\n",
    "fitted_rf_grid_em.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best random forest's accuracy on the training set: 100.0%\n",
      "The best random forest's accuracy on the test set: 46.75925925925926%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best random forest's accuracy on the training set: {fitted_rf_grid_em.score(Xm_train, em_train)*100}%\")\n",
    "print(f\"The best random forest's accuracy on the test set: {fitted_rf_grid_em.score(Xm_test, em_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameter grid for XGBoost (scaling is unnecessary)\n",
    "# xgb_param_grid = [\n",
    "#     # 5 numbers of estimators * 5 max depths = 25 models\n",
    "#     {'scaler': [None], 'dim_reducer': [None], 'model': [XGBClassifier(n_jobs=-1)], 'model__n_estimators': np.arange(100, 501, 100),\n",
    "#      'model__max_depth': np.arange(5, 26, 5)},\n",
    "    \n",
    "#     # 3 PCAs * 5 numbers of estimators * 5 max depths = 75 models\n",
    "#     # I am trying fewer PCAs for XGBoost\n",
    "#     {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': [200, 250, 300], 'model': [XGBClassifier(n_jobs=-1)],\n",
    "#      'model__n_estimators': np.arange(100, 501, 100), 'model__max_depth': np.arange(5, 26, 5)}\n",
    "# ]\n",
    "\n",
    "# # Instantiate the XGB grid search\n",
    "# xgb_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=xgb_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  7.9min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 53.2min\n"
     ]
    }
   ],
   "source": [
    "# # Fit the XGB grid search\n",
    "# fitted_xgb_grid_em = xgb_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above never finished so I decided to comment it out. I will try again without passing `n_jobs=-1` into `XGBClassifier()`, and with a higher number (10 instead of 5) for `verbose` in `GridSearchCV()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for XGBoost (scaling is unnecessary)\n",
    "xgb_param_grid = [\n",
    "    # 5 numbers of estimators * 5 max depths = 25 models\n",
    "    {'scaler': [None], 'dim_reducer': [None], 'model': [XGBClassifier()], 'model__n_estimators': np.arange(100, 501, 100),\n",
    "     'model__max_depth': np.arange(5, 26, 5)},\n",
    "    \n",
    "    # 3 PCAs * 5 numbers of estimators * 5 max depths = 75 models\n",
    "    # I am trying fewer PCAs for XGBoost\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': [200, 250, 300], 'model': [XGBClassifier()],\n",
    "     'model__n_estimators': np.arange(100, 501, 100), 'model__max_depth': np.arange(5, 26, 5)}\n",
    "]\n",
    "\n",
    "# Instantiate the XGB grid search\n",
    "xgb_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=xgb_param_grid, cv=5, n_jobs=-1, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  8.1min\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed: 14.4min\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 27.0min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 34.9min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed: 44.6min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed: 52.7min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed: 65.0min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed: 75.2min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed: 87.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed: 101.2min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed: 105.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 108.7min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 110.1min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 111.9min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 117.4min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 124.6min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 132.7min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 141.9min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 151.0min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 160.9min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 170.8min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 179.5min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 183.5min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 188.1min\n",
      "[Parallel(n_jobs=-1)]: Done 500 out of 500 | elapsed: 191.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the XGB grid search\n",
    "fitted_xgb_grid_em = xgb_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmp3e5mb8m1',\n",
       "         steps=[('scaler', None), ('dim_reducer', None),\n",
       "                ('model',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "                               max_delta_step=0, max_depth=5,\n",
       "                               min_child_weight=1, missing=None,\n",
       "                               n_estimators=300, n_jobs=1, nthread=None,\n",
       "                               objective='multi:softprob', random_state=0,\n",
       "                               reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "                               seed=None, silent=None, subsample=1,\n",
       "                               verbosity=1))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best XGB model?\n",
    "fitted_xgb_grid_em.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best XGB model's accuracy on the training set: 100.0%\n",
      "The best XGB model's accuracy on the test set: 51.388888888888886%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best XGB model's accuracy on the training set: {fitted_xgb_grid_em.score(Xm_train, em_train)*100}%\")\n",
    "print(f\"The best XGB model's accuracy on the test set: {fitted_xgb_grid_em.score(Xm_test, em_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for KNN\n",
    "knn_param_grid = [\n",
    "    # unscaled and scaled * 10 Ks = 20 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [KNeighborsClassifier(n_jobs=-1)], 'model__n_neighbors': np.arange(3, 22, 2)},\n",
    "    \n",
    "    # unscaled\n",
    "    # 5 PCAs * 10 Ks = 50 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 251, 50), 'model': [KNeighborsClassifier(n_jobs=-1)],\n",
    "     'model__n_neighbors': np.arange(3, 22, 2)},\n",
    "    \n",
    "    # scaled\n",
    "    # 4 PCAs * 10 Ks = 40 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50), 'model': [KNeighborsClassifier(n_jobs=-1)],\n",
    "     'model__n_neighbors': np.arange(3, 22, 2)}\n",
    "]\n",
    "\n",
    "# Instantiate the grid search\n",
    "knn_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=knn_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 110 candidates, totalling 550 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    6.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   13.6s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   31.2s\n",
      "[Parallel(n_jobs=-1)]: Done 550 out of 550 | elapsed:   38.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the KNN grid search\n",
    "fitted_knn_grid_em = knn_grid_search.fit(Xm_train, em_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmplqvl750x',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=350,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=-1, n_neighbors=3, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best KNN model?\n",
    "fitted_knn_grid_em.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best KNN model's accuracy on the training set: 76.29482071713147%\n",
      "The best KNN model's accuracy on the test set: 45.370370370370374%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best KNN model's accuracy on the training set: {fitted_knn_grid_em.score(Xm_train, em_train)*100}%\")\n",
    "print(f\"The best KNN model's accuracy on the test set: {fitted_knn_grid_em.score(Xm_test, em_test)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for classifying emotions for males\n",
    "- Of the five classifier types I tried in my grid searches, SVM had the highest accuracy on the test set (60.19%), followed by logistic regression (58.80%), XGBoost (51.39%), random forest (46.76%), and lastly, KNN (45.37%).\n",
    "    - Based on these results, I have pickled the best SVM and logistic regression. In part 4, I will try them on a new, male-only dataset.\n",
    "- Except for the best KNN model, all the best models found in the grid searches had training accuracies of 100%, indicating that they overfit to the training set.\n",
    "    - The best KNN model had a training accuracy of 76.29%, but this was still much higher than its test accuracy of 45.37%.\n",
    "- For the classifier types in which scaling the features matters (logistic regression, SVM, and KNN), all the best models made use of the standard scaler.\n",
    "- Of the five best-in-type models, random forest and KNN were the only two which made use of principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Building Models for Classifying Emotion for Females\n",
    "I will follow the same steps I took in classifying emotions for males, with one difference: This time I will not try XGBoost, due to its long computation time and comparatively low performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>2592</th>\n",
       "      <th>2593</th>\n",
       "      <th>2594</th>\n",
       "      <th>2595</th>\n",
       "      <th>2596</th>\n",
       "      <th>2597</th>\n",
       "      <th>2598</th>\n",
       "      <th>2599</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>-827.634966</td>\n",
       "      <td>-819.774917</td>\n",
       "      <td>-822.853080</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>-828.681928</td>\n",
       "      <td>-828.806285</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.746479</td>\n",
       "      <td>-3.883217</td>\n",
       "      <td>-2.345316</td>\n",
       "      <td>-1.983975</td>\n",
       "      <td>-1.184772</td>\n",
       "      <td>-0.433230</td>\n",
       "      <td>-0.065441</td>\n",
       "      <td>1.982603</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.463962</td>\n",
       "      <td>-821.123676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.900729</td>\n",
       "      <td>-0.312385</td>\n",
       "      <td>0.727722</td>\n",
       "      <td>-2.513208</td>\n",
       "      <td>-1.212583</td>\n",
       "      <td>-0.530722</td>\n",
       "      <td>-0.313061</td>\n",
       "      <td>-0.700614</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-794.177863</td>\n",
       "      <td>-794.177863</td>\n",
       "      <td>-794.177863</td>\n",
       "      <td>-793.894139</td>\n",
       "      <td>-793.137717</td>\n",
       "      <td>-793.468316</td>\n",
       "      <td>-789.533197</td>\n",
       "      <td>-779.010582</td>\n",
       "      <td>-784.943109</td>\n",
       "      <td>-794.177863</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.176496</td>\n",
       "      <td>-0.186471</td>\n",
       "      <td>-0.119524</td>\n",
       "      <td>1.143040</td>\n",
       "      <td>2.358768</td>\n",
       "      <td>1.311458</td>\n",
       "      <td>-2.696237</td>\n",
       "      <td>-2.349113</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-821.041970</td>\n",
       "      <td>-821.041970</td>\n",
       "      <td>-821.041970</td>\n",
       "      <td>-821.041970</td>\n",
       "      <td>-821.041970</td>\n",
       "      <td>-797.509476</td>\n",
       "      <td>-764.372126</td>\n",
       "      <td>-758.265524</td>\n",
       "      <td>-749.010558</td>\n",
       "      <td>-735.980862</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.704384</td>\n",
       "      <td>-4.045181</td>\n",
       "      <td>-1.553333</td>\n",
       "      <td>-0.249181</td>\n",
       "      <td>0.307431</td>\n",
       "      <td>-0.283150</td>\n",
       "      <td>-0.663181</td>\n",
       "      <td>0.141858</td>\n",
       "      <td>female</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-840.787028</td>\n",
       "      <td>-838.701370</td>\n",
       "      <td>-837.959716</td>\n",
       "      <td>-835.558702</td>\n",
       "      <td>-831.944898</td>\n",
       "      <td>-831.552856</td>\n",
       "      <td>-834.229278</td>\n",
       "      <td>-831.681721</td>\n",
       "      <td>-830.127033</td>\n",
       "      <td>-821.357521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259985</td>\n",
       "      <td>0.135009</td>\n",
       "      <td>0.584766</td>\n",
       "      <td>2.333742</td>\n",
       "      <td>2.015384</td>\n",
       "      <td>1.571279</td>\n",
       "      <td>-0.142996</td>\n",
       "      <td>0.755337</td>\n",
       "      <td>female</td>\n",
       "      <td>calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>715</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>-764.912613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.896832</td>\n",
       "      <td>-0.392337</td>\n",
       "      <td>-0.061991</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>716</td>\n",
       "      <td>-735.208853</td>\n",
       "      <td>-735.208853</td>\n",
       "      <td>-735.208853</td>\n",
       "      <td>-735.208853</td>\n",
       "      <td>-735.208853</td>\n",
       "      <td>-731.395753</td>\n",
       "      <td>-712.637540</td>\n",
       "      <td>-671.795962</td>\n",
       "      <td>-635.474418</td>\n",
       "      <td>-604.998766</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286503</td>\n",
       "      <td>-0.015891</td>\n",
       "      <td>-0.268399</td>\n",
       "      <td>0.018467</td>\n",
       "      <td>0.344886</td>\n",
       "      <td>0.272358</td>\n",
       "      <td>-0.009525</td>\n",
       "      <td>-0.550906</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>717</td>\n",
       "      <td>-745.345684</td>\n",
       "      <td>-745.345684</td>\n",
       "      <td>-745.345684</td>\n",
       "      <td>-745.137894</td>\n",
       "      <td>-743.758475</td>\n",
       "      <td>-731.279273</td>\n",
       "      <td>-706.786137</td>\n",
       "      <td>-685.991290</td>\n",
       "      <td>-648.907013</td>\n",
       "      <td>-609.751760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214940</td>\n",
       "      <td>0.425808</td>\n",
       "      <td>-0.073396</td>\n",
       "      <td>-0.160047</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.298043</td>\n",
       "      <td>-1.484139</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>718</td>\n",
       "      <td>-718.295965</td>\n",
       "      <td>-713.528072</td>\n",
       "      <td>-659.640905</td>\n",
       "      <td>-587.309927</td>\n",
       "      <td>-532.164434</td>\n",
       "      <td>-496.112793</td>\n",
       "      <td>-474.472000</td>\n",
       "      <td>-450.585750</td>\n",
       "      <td>-436.910862</td>\n",
       "      <td>-435.459635</td>\n",
       "      <td>...</td>\n",
       "      <td>1.417690</td>\n",
       "      <td>1.811539</td>\n",
       "      <td>2.570258</td>\n",
       "      <td>2.225135</td>\n",
       "      <td>1.471269</td>\n",
       "      <td>1.731574</td>\n",
       "      <td>1.927233</td>\n",
       "      <td>3.116465</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>719</td>\n",
       "      <td>-507.310718</td>\n",
       "      <td>-508.228437</td>\n",
       "      <td>-522.397979</td>\n",
       "      <td>-525.523930</td>\n",
       "      <td>-538.149083</td>\n",
       "      <td>-556.676510</td>\n",
       "      <td>-577.762897</td>\n",
       "      <td>-602.072596</td>\n",
       "      <td>-629.169621</td>\n",
       "      <td>-654.514356</td>\n",
       "      <td>...</td>\n",
       "      <td>1.873972</td>\n",
       "      <td>2.679739</td>\n",
       "      <td>1.604825</td>\n",
       "      <td>1.571852</td>\n",
       "      <td>2.094518</td>\n",
       "      <td>4.062167</td>\n",
       "      <td>3.126861</td>\n",
       "      <td>0.164112</td>\n",
       "      <td>female</td>\n",
       "      <td>surprised</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows Ã— 2602 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0           1           2           3           4           5  \\\n",
       "0   -828.806285 -828.806285 -827.634966 -819.774917 -822.853080 -828.806285   \n",
       "1   -821.463962 -821.463962 -821.463962 -821.463962 -821.463962 -821.463962   \n",
       "2   -794.177863 -794.177863 -794.177863 -793.894139 -793.137717 -793.468316   \n",
       "3   -821.041970 -821.041970 -821.041970 -821.041970 -821.041970 -797.509476   \n",
       "4   -840.787028 -838.701370 -837.959716 -835.558702 -831.944898 -831.552856   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "715 -764.912613 -764.912613 -764.912613 -764.912613 -764.912613 -764.912613   \n",
       "716 -735.208853 -735.208853 -735.208853 -735.208853 -735.208853 -731.395753   \n",
       "717 -745.345684 -745.345684 -745.345684 -745.137894 -743.758475 -731.279273   \n",
       "718 -718.295965 -713.528072 -659.640905 -587.309927 -532.164434 -496.112793   \n",
       "719 -507.310718 -508.228437 -522.397979 -525.523930 -538.149083 -556.676510   \n",
       "\n",
       "              6           7           8           9  ...      2592      2593  \\\n",
       "0   -828.806285 -828.806285 -828.681928 -828.806285  ... -1.746479 -3.883217   \n",
       "1   -821.463962 -821.463962 -821.463962 -821.123676  ... -0.900729 -0.312385   \n",
       "2   -789.533197 -779.010582 -784.943109 -794.177863  ... -4.176496 -0.186471   \n",
       "3   -764.372126 -758.265524 -749.010558 -735.980862  ... -5.704384 -4.045181   \n",
       "4   -834.229278 -831.681721 -830.127033 -821.357521  ...  0.259985  0.135009   \n",
       "..          ...         ...         ...         ...  ...       ...       ...   \n",
       "715 -764.912613 -764.912613 -764.912613 -764.912613  ... -0.896832 -0.392337   \n",
       "716 -712.637540 -671.795962 -635.474418 -604.998766  ... -0.286503 -0.015891   \n",
       "717 -706.786137 -685.991290 -648.907013 -609.751760  ...  0.214940  0.425808   \n",
       "718 -474.472000 -450.585750 -436.910862 -435.459635  ...  1.417690  1.811539   \n",
       "719 -577.762897 -602.072596 -629.169621 -654.514356  ...  1.873972  2.679739   \n",
       "\n",
       "         2594      2595      2596      2597      2598      2599  Gender  \\\n",
       "0   -2.345316 -1.983975 -1.184772 -0.433230 -0.065441  1.982603  female   \n",
       "1    0.727722 -2.513208 -1.212583 -0.530722 -0.313061 -0.700614  female   \n",
       "2   -0.119524  1.143040  2.358768  1.311458 -2.696237 -2.349113  female   \n",
       "3   -1.553333 -0.249181  0.307431 -0.283150 -0.663181  0.141858  female   \n",
       "4    0.584766  2.333742  2.015384  1.571279 -0.142996  0.755337  female   \n",
       "..        ...       ...       ...       ...       ...       ...     ...   \n",
       "715 -0.061991  0.000000  0.000000  0.000000  0.000000  0.000000  female   \n",
       "716 -0.268399  0.018467  0.344886  0.272358 -0.009525 -0.550906  female   \n",
       "717 -0.073396 -0.160047  0.000000  0.000000 -0.298043 -1.484139  female   \n",
       "718  2.570258  2.225135  1.471269  1.731574  1.927233  3.116465  female   \n",
       "719  1.604825  1.571852  2.094518  4.062167  3.126861  0.164112  female   \n",
       "\n",
       "       Emotion  \n",
       "0      neutral  \n",
       "1      neutral  \n",
       "2      neutral  \n",
       "3      neutral  \n",
       "4         calm  \n",
       "..         ...  \n",
       "715  surprised  \n",
       "716  surprised  \n",
       "717  surprised  \n",
       "718  surprised  \n",
       "719  surprised  \n",
       "\n",
       "[720 rows x 2602 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making a new dataframe that contains only female recordings\n",
    "ravdess_mfcc_f_df = ravdess_mfcc_df[ravdess_mfcc_df['Gender'] == 'female'].reset_index().drop('index', axis=1)\n",
    "ravdess_mfcc_f_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataframe into features and target\n",
    "Xf = ravdess_mfcc_f_df.iloc[:, :-2]\n",
    "ef = ravdess_mfcc_f_df['Emotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and test sets\n",
    "Xf_train, Xf_test, ef_train, ef_test = train_test_split(Xf, ef, test_size=0.3, stratify=ef, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(504, 2600)\n",
      "(216, 2600)\n",
      "(504,)\n",
      "(216,)\n"
     ]
    }
   ],
   "source": [
    "# Checking the shapes\n",
    "print(Xf_train.shape)\n",
    "print(Xf_test.shape)\n",
    "print(ef_train.shape)\n",
    "print(ef_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an initial model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy on training set: 100.0%\n",
      "Model accuracy on test set: 68.98148148148148%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "initial_logreg_ef = LogisticRegression()\n",
    "\n",
    "# Fit to training set\n",
    "initial_logreg_ef.fit(Xf_train, ef_train)\n",
    "\n",
    "# Score on training set\n",
    "print(f'Model accuracy on training set: {initial_logreg_ef.score(Xf_train, ef_train)*100}%')\n",
    "\n",
    "# Score on test set\n",
    "print(f'Model accuracy on test set: {initial_logreg_ef.score(Xf_test, ef_test)*100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has overfit to the training set yet again. Interestingly, this initial accuracy on the female test set is noticeably higher than the initial accuracy on the male test set, which was 56.48%. Again, let's evaluate the model further using a confusion matrix and a classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted angry</th>\n",
       "      <th>Predicted calm</th>\n",
       "      <th>Predicted disgusted</th>\n",
       "      <th>Predicted fearful</th>\n",
       "      <th>Predicted happy</th>\n",
       "      <th>Predicted neutral</th>\n",
       "      <th>Predicted sad</th>\n",
       "      <th>Predicted surprised</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Actual angry</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual calm</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual disgusted</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual fearful</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual happy</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual neutral</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual sad</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Actual surprised</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Predicted angry  Predicted calm  Predicted disgusted  \\\n",
       "Actual angry                   20               1                    2   \n",
       "Actual calm                     0              22                    2   \n",
       "Actual disgusted                2               1                   19   \n",
       "Actual fearful                  1               1                    1   \n",
       "Actual happy                    1               0                    0   \n",
       "Actual neutral                  0               1                    1   \n",
       "Actual sad                      2               3                    0   \n",
       "Actual surprised                1               0                    0   \n",
       "\n",
       "                  Predicted fearful  Predicted happy  Predicted neutral  \\\n",
       "Actual angry                      2                3                  0   \n",
       "Actual calm                       1                0                  0   \n",
       "Actual disgusted                  1                0                  0   \n",
       "Actual fearful                   18                1                  0   \n",
       "Actual happy                      5               22                  0   \n",
       "Actual neutral                    0                0                  9   \n",
       "Actual sad                        2                2                  1   \n",
       "Actual surprised                  4                3                  0   \n",
       "\n",
       "                  Predicted sad  Predicted surprised  \n",
       "Actual angry                  1                    0  \n",
       "Actual calm                   4                    0  \n",
       "Actual disgusted              4                    2  \n",
       "Actual fearful                4                    3  \n",
       "Actual happy                  0                    1  \n",
       "Actual neutral                2                    1  \n",
       "Actual sad                   19                    0  \n",
       "Actual surprised              0                   20  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having initial_logreg_ef make predictions based on the test set features\n",
    "ef_pred = initial_logreg_ef.predict(Xf_test)\n",
    "\n",
    "# Building the confusion matrix as a dataframe\n",
    "emotions = ['angry', 'calm', 'disgusted', 'fearful', 'happy', 'neutral', 'sad', 'surprised']\n",
    "ef_confusion_df = pd.DataFrame(confusion_matrix(ef_test, ef_pred))\n",
    "ef_confusion_df.columns = [f'Predicted {emotion}' for emotion in emotions]\n",
    "ef_confusion_df.index = [f'Actual {emotion}' for emotion in emotions]\n",
    "ef_confusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.74      0.69      0.71        29\n",
      "        calm       0.76      0.76      0.76        29\n",
      "   disgusted       0.76      0.66      0.70        29\n",
      "     fearful       0.55      0.62      0.58        29\n",
      "       happy       0.71      0.76      0.73        29\n",
      "     neutral       0.90      0.64      0.75        14\n",
      "         sad       0.56      0.66      0.60        29\n",
      "   surprised       0.74      0.71      0.73        28\n",
      "\n",
      "    accuracy                           0.69       216\n",
      "   macro avg       0.71      0.69      0.70       216\n",
      "weighted avg       0.70      0.69      0.69       216\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report\n",
    "print(classification_report(ef_test, ef_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the initial model is strongest at classifying calm voice clips, and weakest at classifying fearful voice clips. In order of strongest to weakest: calm, neutral, happy, surprised, angry, disgusted, sad, and fearful.\n",
    "\n",
    "There is not as much variance in performance across the emotions when compared to that of the initial model for male emotions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although I found that none of the best male emotion classifiers made use of PCA, I will still examine the explained variance ratios like I did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on unscaled features\n",
    "\n",
    "# Instantiate PCA and fit to Xf_train\n",
    "pca = PCA().fit(Xf_train)\n",
    "\n",
    "# Transform Xf_train\n",
    "Xf_train_pca = pca.transform(Xf_train)\n",
    "\n",
    "# Transform Xf_test\n",
    "Xf_test_pca = pca.transform(Xf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling\n",
    "\n",
    "# Instantiate the scaler and fit to Xf_train\n",
    "scaler = StandardScaler().fit(Xf_train)\n",
    "\n",
    "# Transform Xf_train\n",
    "Xf_train_scaled = scaler.transform(Xf_train)\n",
    "\n",
    "# Transform Xf_test\n",
    "Xf_test_scaled = scaler.transform(Xf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA on scaled features\n",
    "\n",
    "# Instantiate PCA and fit to Xf_train_scaled\n",
    "pca_scaled = PCA().fit(Xf_train_scaled)\n",
    "\n",
    "# Transform Xf_train_scaled\n",
    "Xf_train_scaled_pca = pca_scaled.transform(Xf_train_scaled)\n",
    "\n",
    "# Transform Xf_test_scaled\n",
    "Xf_test_scaled_pca = pca_scaled.transform(Xf_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the explained variance ratios\n",
    "\n",
    "plt.subplots(1, 2, figsize = (15, 5))\n",
    "\n",
    "# Unscaled\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(np.arange(1, len(pca.explained_variance_ratio_)+1), pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA on Unscaled Features')\n",
    "plt.ylim(top = 0.5) # Equalizing the y-axes\n",
    "\n",
    "# Scaled\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(np.arange(1, len(pca_scaled.explained_variance_ratio_)+1), pca_scaled.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA on Scaled Features')\n",
    "plt.ylim(top = 0.5) # Equalizing the y-axes\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the same trends I saw previously for male emotions.\n",
    "\n",
    "How much variance is explained by certain numbers of unscaled and scaled principal components? This will help me determine how many principal components to try in my grid searches later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by 502 unscaled principal components: 100.0%\n",
      "Variance explained by 450 unscaled principal components: 99.91%\n",
      "Variance explained by 400 unscaled principal components: 99.77%\n",
      "Variance explained by 350 unscaled principal components: 99.56%\n",
      "Variance explained by 300 unscaled principal components: 99.25%\n",
      "Variance explained by 250 unscaled principal components: 98.77%\n",
      "Variance explained by 200 unscaled principal components: 98.05%\n",
      "Variance explained by 150 unscaled principal components: 96.91%\n",
      "Variance explained by 100 unscaled principal components: 94.95%\n",
      "Variance explained by 50 unscaled principal components: 90.6%\n"
     ]
    }
   ],
   "source": [
    "# Unscaled\n",
    "num_components = [503, 451, 401, 351, 301, 251, 201, 151, 101, 51]\n",
    "for n in num_components:\n",
    "    print(f'Variance explained by {n-1} unscaled principal components: {np.round(np.sum(pca.explained_variance_ratio_[:n])*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained by 502 scaled principal components: 100.0%\n",
      "Variance explained by 450 scaled principal components: 99.5%\n",
      "Variance explained by 400 scaled principal components: 98.7%\n",
      "Variance explained by 350 scaled principal components: 97.51%\n",
      "Variance explained by 300 scaled principal components: 95.82%\n",
      "Variance explained by 250 scaled principal components: 93.39%\n",
      "Variance explained by 200 scaled principal components: 89.88%\n",
      "Variance explained by 150 scaled principal components: 84.69%\n",
      "Variance explained by 100 scaled principal components: 76.61%\n",
      "Variance explained by 50 scaled principal components: 62.4%\n"
     ]
    }
   ],
   "source": [
    "# Scaled\n",
    "num_components = [503, 451, 401, 351, 301, 251, 201, 151, 101, 51]\n",
    "for n in num_components:\n",
    "    print(f'Variance explained by {n-1} scaled principal components: {np.round(np.sum(pca_scaled.explained_variance_ratio_[:n])*100, 2)}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, I will now do a grid search for each classifier type, with five-fold cross-validation to optimize the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache\n",
    "cachedir = mkdtemp()\n",
    "\n",
    "# Pipeline (these values are placeholders)\n",
    "my_pipeline = Pipeline(steps=[('scaler', StandardScaler()), ('dim_reducer', PCA()), ('model', LogisticRegression())], memory=cachedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for log reg\n",
    "logreg_param_grid = [\n",
    "    # l1 without PCA\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [LogisticRegression(penalty='l1', n_jobs=-1)],\n",
    "     'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l1 unscaled with PCA\n",
    "    # 6 PCAs * 9 regularization strengths = 54 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 301, 50),\n",
    "     'model': [LogisticRegression(penalty='l1', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l1 scaled with PCA\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50),\n",
    "     'model': [LogisticRegression(penalty='l1', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) without PCA\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)],\n",
    "     'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) unscaled with PCA\n",
    "    # 6 PCAs * 9 regularization strengths = 54 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 301, 50),\n",
    "     'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # l2 (default) scaled with PCA\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50),\n",
    "     'model': [LogisticRegression(solver='lbfgs', n_jobs=-1)], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "]\n",
    "\n",
    "# Instantiate the log reg grid search\n",
    "logreg_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=logreg_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   35.6s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   53.8s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed:  4.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the log reg grid search\n",
    "fitted_logreg_grid_ef = logreg_grid_search.fit(Xf_train, ef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmpu3y2toc2',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=350,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 LogisticRegression(C=0.1, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=-1, penalty='l2',\n",
       "                                    random_state=None, solver='lbfgs',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best log reg?\n",
    "fitted_logreg_grid_ef.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best log reg's accuracy on the training set: 100.0%\n",
      "The best log reg's accuracy on the test set: 71.29629629629629%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best log reg's accuracy on the training set: {fitted_logreg_grid_ef.score(Xf_train, ef_train)*100}%\")\n",
    "print(f\"The best log reg's accuracy on the test set: {fitted_logreg_grid_ef.score(Xf_test, ef_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for SVM\n",
    "svm_param_grid = [\n",
    "    # unscaled and scaled * 9 regularization strengths = 18 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [SVC()], 'model__C': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # unscaled\n",
    "    # 6 PCAs * 9 regularization strengths = 54 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 301, 50), 'model': [SVC()],\n",
    "     'model__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]},\n",
    "    \n",
    "    # scaled\n",
    "    # 4 PCAs * 9 regularization strengths = 36 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50), 'model': [SVC()],\n",
    "     'model__C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "]\n",
    "\n",
    "# Instantiate the SVM grid search\n",
    "svm_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=svm_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   11.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   34.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   49.4s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   57.9s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the SVM grid search\n",
    "fitted_svm_grid_ef = svm_grid_search.fit(Xf_train, ef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmpu3y2toc2',\n",
       "         steps=[('scaler',\n",
       "                 StandardScaler(copy=True, with_mean=True, with_std=True)),\n",
       "                ('dim_reducer', None),\n",
       "                ('model',\n",
       "                 SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "                     decision_function_shape='ovr', degree=3,\n",
       "                     gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
       "                     probability=False, random_state=None, shrinking=True,\n",
       "                     tol=0.001, verbose=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best SVM?\n",
    "fitted_svm_grid_ef.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best SVM's accuracy on the training set: 100.0%\n",
      "The best SVM's accuracy on the test set: 70.83333333333334%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best SVM's accuracy on the training set: {fitted_svm_grid_ef.score(Xf_train, ef_train)*100}%\")\n",
    "print(f\"The best SVM's accuracy on the test set: {fitted_svm_grid_ef.score(Xf_test, ef_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for random forest (scaling is unnecessary)\n",
    "rf_param_grid = [\n",
    "    # 5 numbers of estimators * 5 max depths = 25 models\n",
    "    {'scaler': [None], 'dim_reducer': [None], 'model': [RandomForestClassifier(n_jobs=-1)], 'model__n_estimators': np.arange(100, 501, 100),\n",
    "     'model__max_depth': np.arange(5, 26, 5)},\n",
    "    \n",
    "    # 6 PCAs * 5 numbers of estimators * 5 max depths = 150 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 301, 50), 'model': [RandomForestClassifier(n_jobs=-1)],\n",
    "     'model__n_estimators': np.arange(100, 501, 100), 'model__max_depth': np.arange(5, 26, 5)}\n",
    "]\n",
    "\n",
    "# Instantiate the rf grid search\n",
    "rf_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=rf_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 175 candidates, totalling 875 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 875 out of 875 | elapsed:  8.6min finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the rf grid search\n",
    "fitted_rf_grid_ef = rf_grid_search.fit(Xf_train, ef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmpu3y2toc2',\n",
       "         steps=[('scaler', None),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=150,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=20,\n",
       "                                        max_features='auto',\n",
       "                                        max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=1, min_samples_split=2,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=500, n_jobs=-1,\n",
       "                                        oob_score=False, random_state=None,\n",
       "                                        verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best rf?\n",
    "fitted_rf_grid_ef.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best random forest's accuracy on the training set: 100.0%\n",
      "The best random forest's accuracy on the test set: 61.57407407407407%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best random forest's accuracy on the training set: {fitted_rf_grid_ef.score(Xf_train, ef_train)*100}%\")\n",
    "print(f\"The best random forest's accuracy on the test set: {fitted_rf_grid_ef.score(Xf_test, ef_test)*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid for KNN\n",
    "knn_param_grid = [\n",
    "    # unscaled and scaled * 10 Ks = 20 models\n",
    "    {'scaler': [None, StandardScaler()], 'dim_reducer': [None], 'model': [KNeighborsClassifier(n_jobs=-1)], 'model__n_neighbors': np.arange(3, 22, 2)},\n",
    "    \n",
    "    # unscaled\n",
    "    # 6 PCAs * 10 Ks = 60 models\n",
    "    {'scaler': [None], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(50, 301, 50), 'model': [KNeighborsClassifier(n_jobs=-1)],\n",
    "     'model__n_neighbors': np.arange(3, 22, 2)},\n",
    "    \n",
    "    # scaled\n",
    "    # 4 PCAs * 10 Ks = 40 models\n",
    "    {'scaler': [StandardScaler()], 'dim_reducer': [PCA()], 'dim_reducer__n_components': np.arange(200, 351, 50), 'model': [KNeighborsClassifier(n_jobs=-1)],\n",
    "     'model__n_neighbors': np.arange(3, 22, 2)}\n",
    "]\n",
    "\n",
    "# Instantiate the grid search\n",
    "knn_grid_search = GridSearchCV(estimator=my_pipeline, param_grid=knn_param_grid, cv=5, n_jobs=-1, verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 120 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:    7.3s\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:   14.7s\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:   22.1s\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:   32.7s\n",
      "[Parallel(n_jobs=-1)]: Done 600 out of 600 | elapsed:   44.3s finished\n"
     ]
    }
   ],
   "source": [
    "# Fit the KNN grid search\n",
    "fitted_knn_grid_ef = knn_grid_search.fit(Xf_train, ef_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory='C:\\\\Users\\\\Patrick\\\\AppData\\\\Local\\\\Temp\\\\tmpu3y2toc2',\n",
       "         steps=[('scaler', None),\n",
       "                ('dim_reducer',\n",
       "                 PCA(copy=True, iterated_power='auto', n_components=100,\n",
       "                     random_state=None, svd_solver='auto', tol=0.0,\n",
       "                     whiten=False)),\n",
       "                ('model',\n",
       "                 KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
       "                                      metric='minkowski', metric_params=None,\n",
       "                                      n_jobs=-1, n_neighbors=15, p=2,\n",
       "                                      weights='uniform'))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What was the best KNN model?\n",
    "fitted_knn_grid_ef.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best KNN model's accuracy on the training set: 59.32539682539682%\n",
      "The best KNN model's accuracy on the test set: 55.55555555555556%\n"
     ]
    }
   ],
   "source": [
    "print(f\"The best KNN model's accuracy on the training set: {fitted_knn_grid_ef.score(Xf_train, ef_train)*100}%\")\n",
    "print(f\"The best KNN model's accuracy on the test set: {fitted_knn_grid_ef.score(Xf_test, ef_test)*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions for classifying emotions for females\n",
    "- Of the four classifier types I tried in my grid searches, logistic regression had the highest accuracy on the test set (71.29%), followed by SVM (70.83%), random forest (61.57%), and lastly, KNN (55.56%).\n",
    "- Except for the best KNN model, all the best models found in the grid searches had training accuracies of 100%, indicating that they overfit to the training set.\n",
    "    - The best KNN model had a training accuracy of 59.33%, which was not much higher than its test accuracy of 55.56%. A much wider gap was found in the best KNN model for male emotions.\n",
    "- For the classifier types in which scaling the features matters (logistic regression, SVM, and KNN), the best logistic regression and SVM models made use of the standard scaler, while the best KNN model did not.\n",
    "- All the best-in-type models made use of principal components, except SVM.\n",
    "- Interestingly, the female emotion classifiers achieved higher accuracies than their male counterparts. It appears that for the RAVDESS dataset, the differences between female emotions are greater the differences between male emotions.\n",
    "    - Based on this alone, I cannot extrapolate and conclude that women are more socially more expressive than men are, although this is an interesting thought."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
